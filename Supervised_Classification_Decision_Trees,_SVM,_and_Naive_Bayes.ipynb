{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1: What is Information Gain, and how is it used in Decision Trees?\n",
        "- Information Gain measures the reduction in entropy or surprise by splitting the dataset according to a given attribute. In decision trees, it helps in selecting the attribute that best separates the samples into target classes at each node. The attribute with the highest information gain is chosen for the split, resulting in more informative and pure child nodes."
      ],
      "metadata": {
        "id": "9-KgfwGTxfpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: What is the difference between Gini Impurity and Entropy?\n",
        "-  Gini Impurity quantifies the probability of incorrectly classifying a randomly chosen element if it is labeled according to the distribution of labels in the dataset.\n",
        "\n",
        "     Entropy measures the average amount of information or uncertainty in the dataset.\n",
        "\n",
        "     Gini is computationally less intensive and is preferred when speed is important, while Entropy (and thus Information Gain) is more theoretically sound and is often used when theoretical purity is important"
      ],
      "metadata": {
        "id": "RPKgiLU2xzUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: What is Pre-Pruning in Decision Trees?\n",
        "-   Pre-pruning is a technique that stops the tree's growth early by setting conditions (like maximum depth, minimum samples per leaf, or minimum samples per split) to prevent overfitting. It keeps the model simpler and more generalizable by avoiding the creation of very specific rules from the training data."
      ],
      "metadata": {
        "id": "Yd2ScEgkyIqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "Pz2AGubmyZ7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIkJT8toyoyz",
        "outputId": "b76190f3-b8cc-4be3-f273-7200db0664b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances: [0.02666667 0.         0.55072262 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: What is a Support Vector Machine (SVM)?\n",
        "-   A Support Vector Machine is a supervised learning algorithm used for classification and regression. It finds the hyperplane that best separates data points of different classes by maximizing the margin between the closest points (called support vectors) of each class"
      ],
      "metadata": {
        "id": "RwbbdFUky2Cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: What is the Kernel Trick in SVM?\n",
        "-   The Kernel Trick allows SVM to find nonlinear decision boundaries by implicitly mapping input features into higher-dimensional spaces without explicit computation. This enables the classifier to solve problems that are not linearly separable in the original feature space.\n"
      ],
      "metadata": {
        "id": "nxvBnx6ZzQTE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "npq7l7kYzmlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, random_state=42)\n",
        "\n",
        "# Linear kernel SVM\n",
        "svc_linear = SVC(kernel='linear')\n",
        "svc_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svc_linear.predict(X_test))\n",
        "\n",
        "# RBF kernel SVM\n",
        "svc_rbf = SVC(kernel='rbf')\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svc_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrET3fdwzxQ7",
        "outputId": "bbd0b628-9d08-4396-9d34-6c506a0073d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9777777777777777\n",
            "RBF Kernel Accuracy: 0.7111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "-   Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem, assuming independence among predictors. It is called “naïve” because it assumes that all features are independent of each other, an assumption that is rarely true in real data"
      ],
      "metadata": {
        "id": "XK9pt_I-z5Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "-   Gaussian Naïve Bayes: Used for continuous data and assumes features follow a normal distribution.\n",
        "\n",
        "     Multinomial Naïve Bayes: Works best with discrete counts, e.g., word counts in text classification.\n",
        "\n",
        "     Bernoulli Naïve Bayes: Designed for binary/boolean features (present/absent).\n"
      ],
      "metadata": {
        "id": "ab309ueR0NFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "tlHwho7f0h27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhTMkpUa0tEj",
        "outputId": "f7e6b31f-85b1-4ff9-f1c0-11273f06b371"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.958041958041958\n"
          ]
        }
      ]
    }
  ]
}